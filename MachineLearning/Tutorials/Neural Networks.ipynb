{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The following Notebook is about Neural Networks Implemented from scratch! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import relevant libraries\n",
    "import bonnerlib2D as bl2d\n",
    "import numpy as np\n",
    "import pickle as pickle\n",
    "import matplotlib.pyplot as plt\n",
    "import sklearn.linear_model as lin\n",
    "import sklearn.neural_network as nn\n",
    "import sklearn.utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In this tutorial, we work with the MNIST data\n",
    "with open(\"/home/tahir/Desktop/Datasets/mnistTVT.pickle\", \"rb\") as f:\n",
    "    Xtrain, Ttrain, Xval, Tval, Xtest, Ttest = pickle.load(f)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Networks with sklearn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will make this neural net with 1 hidden unit, maximum of 100 epochs of training <br>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Test Accuracy is:  0.4456\n",
      "The Test Accuracy with 1 Hidden layer is:  0.4456\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# set seed for reproducability \n",
    "np.random.seed(0)  #set seed for reproducability \n",
    "\n",
    "def MLP_Classifier(Xtrain, Ttrain, Xtest, Ttest, hidden_layers, initial_lr, max_iters):\n",
    "    \"\"\"\n",
    "    Given training data and testing data along with a few other parameters of the MLP classifier,\n",
    "    compute the MLP classification and return the accuracy of the model. \n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    Xtrain : X values of the training set.\n",
    "    Ttrain : T values of the training set.\n",
    "    Xtest : X values of the testing set.\n",
    "    Ttest : T values of the testing set.\n",
    "    hidden_layers : A tuple of the number of layers.\n",
    "    initial_lr : The initial learning rate value. \n",
    "    max_iterations : The training iteration values. \n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    accuracy : The score of how accurate was the MLP classifier. \n",
    "\n",
    "    \"\"\"\n",
    "    clf = nn.MLPClassifier(activation=\"logistic\",\n",
    "                        solver=\"sgd\", \n",
    "                        hidden_layer_sizes=(hidden_layers), \n",
    "                        learning_rate_init=initial_lr,\n",
    "                        tol=10e-6,\n",
    "                        max_iter=max_iters)\n",
    "\n",
    "    clf.fit(Xtrain,Ttrain)\n",
    "    accuracy  = clf.score(Xtest,Ttest)\n",
    "    print(\"The Test Accuracy is: \", accuracy)\n",
    "\n",
    "    return accuracy\n",
    "\n",
    "MLP1 = MLP_Classifier(Xtrain, Ttrain, Xtest, Ttest, hidden_layers=(1,), initial_lr=0.01, max_iters=1000)\n",
    "print(\"The Test Accuracy with 1 Hidden layer is: \", MLP1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Test Accuracy is:  0.6979\n",
      "The Test Accuracy with 2 Hidden layer is:  0.6979\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Lets try it bunch of different hidden layers\n",
    "\n",
    "# 2 Hidden layers\n",
    "MLP2 = MLP_Classifier(Xtrain, Ttrain, Xtest, Ttest, hidden_layers=(2,), initial_lr=0.01, max_iters=1000)\n",
    "plt.suptitle(\"Question 3(c): Neural Net with 2 hidden unit\")\n",
    "print(\"The Test Accuracy with 2 Hidden layer is: \", MLP2)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Test Accuracy is:  0.93\n",
      "The Test Accuracy with 9 Hidden layer is:  0.93\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tahir/anaconda3/lib/python3.8/site-packages/sklearn/neural_network/_multilayer_perceptron.py:582: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 9 Hidden Layers\n",
    "MLP3 = MLP_Classifier(Xtrain, Ttrain, Xtest, Ttest, hidden_layers=(9,), initial_lr=0.01, max_iters=1000)\n",
    "plt.suptitle(\"Question 3(d): Neural Net with 9 hidden unit\")\n",
    "print(\"The Test Accuracy with 9 Hidden layer is: \", MLP3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute Accuracy for a Binary Output - Uses a logistic Activation\n",
    "\n",
    "# Helper Function\n",
    "def sigmoid(z):\n",
    "    \"\"\" \n",
    "    Return the sigmoid version of the given equation.\n",
    "    \"\"\"\n",
    "    return 1 / ( 1 + np.exp(-z) )\n",
    "\n",
    "def BinaryNN(clf,X,T):\n",
    "    \"\"\"\n",
    "    Ccomputes and returns the accuracy of classifier clf on data X,T, where clf\n",
    "    is a neural network with one hidden layer.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    clf : The MLP Classification object\n",
    "    X : X values of the testing data.\n",
    "    T : The corresponding true labels of the data.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    The score of how accurate was the MLP Classifier with 1 Hidden layer.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    # Compute the forward propogation \n",
    "    z1 = np.dot(X,clf.coefs_[0]) + clf.intercepts_[0]   # weighted sum of the inputs\n",
    "    h1 = sigmoid(z1)                                    # First hidden layer\n",
    "    z2 = np.dot(h1, clf.coefs_[1]) + clf.intercepts_[1] # weighted sum passed onto next layer \n",
    "    y = np.argmax(z2, axis=1)                           # output layer\n",
    "\n",
    "    return np.mean(y==T)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute Accuracy for a Multiclass Neural Network - Uses Softmax Activation\n",
    "# The loss function in this case is the cross entorpy loss\n",
    "\n",
    "def softmax(z):\n",
    "    \"\"\"\n",
    "    The softmax activate funcion \n",
    "    Return the probability of each class\n",
    "    \"\"\"\n",
    "    denominator = np.sum( np.exp(z), axis=1)\n",
    "    return np.exp(z) / denominator.reshape(denominator.shape[0],1)\n",
    "\n",
    "\n",
    "def ceNN(clf, X, T):\n",
    "    \"\"\"\n",
    "    Compute and return the cross entropy of the MLP classifier in two ways. \n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    clf : The MLP classifier.\n",
    "    X : The X testing points.\n",
    "    T : The corresponding True labels.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    CE_1 : Cross Entropy Loss computed from the sklearn build in methods\n",
    "    CE_2 : Cross Entropy Loss computed from scratch. \n",
    "    \"\"\"\n",
    "    # Method 1\n",
    "    # Get the logarithm of the probabilities for each class. \n",
    "    logProbabilities = clf.predict_log_proba(X) \n",
    "\n",
    "    # Encode labels a one hot vector  \t   # We use np.unique() to be able to do\n",
    "    labels = np.eye(len(np.unique(T)))[T]  # this for any # of classes\n",
    "\n",
    "    # Method 2\n",
    "    # Compute the forward propogation \n",
    "    z1 = np.dot(X,clf.coefs_[0]) + clf.intercepts_[0]   # weighted sum of the inputs\n",
    "    h1 = sigmoid(z1)                                    # First hidden layer\n",
    "    z2 = np.dot(h1, clf.coefs_[1]) + clf.intercepts_[1] # weighted sum passed onto next layer \n",
    "    y = softmax(z2)                                     # Use softmax to get the probability for each class. \n",
    "\n",
    "    # Compute the Cross entropy loss --  \n",
    "    CE1 = np.sum( -labels * logProbabilities ) / len(labels)\n",
    "    CE2 = np.sum( -labels * np.log(y) ) / len(labels)\n",
    "\n",
    "    return CE1, CE2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementing Neural Network from Scratch -- Completely! \n",
    "\n",
    "Do NOT try this at home. <br>\n",
    "\n",
    "P.S; If you do try it, you can reach out to me if you need help :) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in the data File\n",
    "with open(\"/home/tahir/Desktop/Datasets/mnistTVT.pickle\", \"rb\") as f:\n",
    "    Xtrain, Ttrain, Xval, Tval, Xtest, Ttest = pickle.load(f)\n",
    "\n",
    "def get_digits(a,b,TrainData,TestData):\n",
    "    \"\"\"\n",
    "    Given two numbers a,and b, along with 2 datasets -- Train & Test,\n",
    "    Get a reduced dataset with just the specified digits. \n",
    "    Parameters\n",
    "    ----------\n",
    "    a : A digit from 1 to 10.\n",
    "    b : A second digit from 1 to 10.\n",
    "    TrainData: The Training Dataset \n",
    "    TestData: The Testing Dataset\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    Reduced Version of the Data (Features and Labels), which \n",
    "    contains only the digits specified. \n",
    "    \"\"\"\n",
    "    # Get the digit indices from there labels, then get the corresponding rows.\n",
    "    SubsetFeatures = TrainData[(TestData == a ) | (TestData == b)] \n",
    "    # Extract the labels for the specified digits\n",
    "    SubsetLabels =  TestData[(TestData == a) | (TestData == b)]\n",
    "    return SubsetFeatures, SubsetLabels\n",
    "\n",
    "# Get digits 5 and 6 from MNST dataset (subset of the data)\n",
    "sub_Xtrain, sub_Ttrain = get_digits(5,6,Xtrain,Ttrain)  # (9444, 784) & (9444,)\n",
    "sub_Xtest, sub_Ttest = get_digits(5,6,Xtest,Ttest)      # (1850, 784) & (1850,)\n",
    "\n",
    "# Encode the values into 1s and 0s\n",
    "sub_Ttrain = np.where(sub_Ttrain == 5,1,0)\n",
    "sub_Ttest = np.where(sub_Ttest == 5, 1, 0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper Functions\n",
    "\n",
    "def sigmoid(z):\n",
    "    \"\"\" \n",
    "    Return the sigmoid / logistic activation function\n",
    "    \"\"\"\n",
    "    return 1 / ( 1 + np.exp(-z) )\n",
    "\n",
    "def cross_entropy(y,t):\n",
    "    \"\"\" \n",
    "    Return the Cross Entropy loss between predicted y and t \n",
    "    (This is for a binary classification). \n",
    "    \"\"\"\n",
    "    return -t*np.log(y) - (1-t)*np.log(1-y)\n",
    "\n",
    "def tanh(z):\n",
    "    \"\"\"\n",
    "    The tanh activation function\n",
    "    \"\"\"\n",
    "    return (np.exp(z) - np.exp(-z)) / (np.exp(z) + np.exp(-z))\n",
    "\n",
    "def softmax(z):\n",
    "    \"\"\"\n",
    "    The softmax activate funcion \n",
    "    Return the probability of each class\n",
    "    \"\"\"\n",
    "    \n",
    "    return np.exp(z) / np.sum(np.exp(z), axis=0)\n",
    "\n",
    "def get_Acc(o,labels):\n",
    "    \"\"\" \n",
    "    Get the accuracy of the prediction from our MLP classifier \n",
    "    \"\"\"\n",
    "    o = np.squeeze(np.where(o>0.5,1,0))\n",
    "    return np.sum (np.equal(o,labels)) / len(labels) \n",
    "\n",
    "def get_CE(o, T):\n",
    "    \"\"\" \n",
    "    Get the Cross Entropy loss of our prediction from the MLP classifier. \n",
    "    \"\"\"\n",
    "    o = np.squeeze(o) \n",
    "    # Compute the Cross entropy loss \n",
    "    cross_entropy = -T*np.log(o) - (1-T)*np.log(1-o)\n",
    "    return np.mean( cross_entropy )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration:0, Test accuracy: 0.6302702702702703\n",
      "Iteration:1, Test accuracy: 0.6491891891891892\n",
      "Iteration:2, Test accuracy: 0.6616216216216216\n",
      "Iteration:3, Test accuracy: 0.6767567567567567\n",
      "Iteration:4, Test accuracy: 0.6897297297297297\n",
      "Iteration:5, Test accuracy: 0.7037837837837838\n",
      "Iteration:6, Test accuracy: 0.7113513513513513\n",
      "Iteration:7, Test accuracy: 0.72\n",
      "Iteration:8, Test accuracy: 0.7297297297297297\n",
      "Iteration:9, Test accuracy: 0.7367567567567568\n",
      "Iteration:10, Test accuracy: 0.7427027027027027\n",
      " \n",
      "The final Test Accuracy is: 0.7427027027027027\n",
      "The final Cross Entropy loss is: 1.1803255469619718\n"
     ]
    }
   ],
   "source": [
    "# Get digits 5 and 6 from MNST dataset (subset of the data)\n",
    "sub_Xtrain, sub_Ttrain = get_digits(5,6,Xtrain,Ttrain)  # (9444, 784) & (9444,)\n",
    "sub_Xtest, sub_Ttest = get_digits(5,6,Xtest,Ttest)      # (1850, 784) & (1850,)\n",
    "\n",
    "# Get the reduced dataset \n",
    "# Encode the values into 1s and 0s\n",
    "X = sub_Xtrain \n",
    "T = np.where(sub_Ttrain == 5,1,0)\n",
    "Test_X = sub_Xtest\n",
    "Test_T = np.where(sub_Ttest == 5, 1, 0)\n",
    "\n",
    "# Set seed\n",
    "np.random.seed(0)\n",
    "\n",
    "# initialize wieghts\n",
    "W = np.random.normal(0, 1, (sub_Xtrain.shape[1], 100))  # this is of shape (input features, 100)\n",
    "V = np.random.normal(0, 1, (100, 100))  # Hidden layer 1 of shape 100, 100 cz next layer also has 100 \n",
    "U = np.random.normal(0, 1, (100, 1))    # Hidden layer 2 of shape 100, 1 as 1 output (sigmoid) \n",
    "\n",
    "# initialize bais terms same shape as the Wieght matrices they are being added with. \n",
    "w0 = np.zeros(100)\n",
    "v0 = np.zeros(100)\n",
    "u0 = np.zeros(1)\n",
    "\n",
    "# Initialize the learning rate\n",
    "learning_rate = 0.1\n",
    "\n",
    "# Main Gradient Descent Loop\n",
    "for iteration in range(0,11): \n",
    "\n",
    "    # Compute the Forward Pass for Training Data\n",
    "    # Lets just call the ~ on top of variables t or tilda \n",
    "    x_t = np.matmul(X,W) + w0   #(9334, 100) + bais \n",
    "    h = tanh(x_t)               # First hidden layer, tan  # (9334,100)\n",
    "    h_t = np.matmul(h,V) + v0   # weighted sum passed onto next layer  (9334,100)\n",
    "    g = tanh(h_t)               # Second hidden layer, tan activation \n",
    "    g_t = np.matmul(g,U) + u0   # Connecting layer between hidden layer2 and output layer\n",
    "    o = sigmoid(g_t)            # Sigmoid results for binary classification\n",
    "\n",
    "    # Compute the Forward Pass for TESTING ONLY\n",
    "    x_t_test = np.matmul(Test_X,W) + w0    #(9334, 100) + bais \n",
    "    h_test = tanh(x_t_test)                # First hidden layer, tan  # (9334,100)\n",
    "    h_t_test = np.matmul(h_test,V) + v0    # weighted sum passed onto next layer  (9334,100)\n",
    "    g_test = tanh(h_t_test)                # Second hidden layer, tan activation \n",
    "    g_t_test = np.matmul(g_test,U) + u0    # Connecting layer between hidden layer2 and output layer\n",
    "    o_test = np.squeeze(sigmoid(g_t_test)) # Sigmoid results for binary classification\n",
    "\n",
    "\n",
    "    print(f\"Iteration:{ iteration }, Test accuracy: { get_Acc(o_test,Test_T) }\")\n",
    "\n",
    "    # Compute the Backward pass\n",
    "    DC_DGtilda = o - T.reshape(len(T),1)\n",
    "    DC_DU = np.matmul(g.T,DC_DGtilda)  \n",
    "    DC_DG = np.matmul(DC_DGtilda,U.T)\n",
    "    DC_DHtilda = (1 - g**2) * DC_DG\n",
    "    DC_DV = np.matmul(h.T,DC_DHtilda) \n",
    "    DC_DH = np.matmul(DC_DHtilda, V.T)\n",
    "    DC_DXtilda = (1-h**2)*DC_DH\n",
    "    DC_DW = np.matmul(X.T, DC_DXtilda)\n",
    "\n",
    "    # Gradients with respect to the bais\n",
    "    DC_du0 = DC_DGtilda.sum(axis=0)\n",
    "    DC_dv0 = DC_DHtilda.sum(axis=0)\n",
    "    DC_dw0 = DC_DXtilda.sum(axis=0) \n",
    "\n",
    "\n",
    "    # Preform weight updates, using Average Gradient\n",
    "    W -= DC_DW * learning_rate / sub_Xtrain.shape[0]\n",
    "    V -= DC_DV * learning_rate / sub_Xtrain.shape[0]\n",
    "    U -= DC_DU * learning_rate / sub_Xtrain.shape[0]\n",
    "\n",
    "    # Update the bais term \n",
    "    w0 -= DC_dw0 * learning_rate / sub_Xtrain.shape[0]\n",
    "    v0 -= DC_dv0 * learning_rate / sub_Xtrain.shape[0]\n",
    "    u0 -= DC_du0 * learning_rate / sub_Xtrain.shape[0] \n",
    "\n",
    "print(\" \")\n",
    "print(f\"The final Test Accuracy is: { get_Acc(o_test,Test_T) }\") \n",
    "print(f\"The final Cross Entropy loss is: {get_CE(o_test,Test_T)}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
